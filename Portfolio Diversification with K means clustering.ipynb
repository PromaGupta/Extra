{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb11574e",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304369f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import kneed\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets \n",
    "from scipy.stats import mstats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import ConnectionError\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcbbf1",
   "metadata": {},
   "source": [
    "<a name=\"definition\"></a>\n",
    "<h2><span style=\"color:black\">What is K-Means Clustering?  </span></h2>\n",
    "<p>\n",
    "K-Means Clustering is a form of unsupervised <a href=\"https://hdonnelly6.medium.com/list/machine-learning-for-investing-7f2690bb1826\">machine learning</a> (ML). It is considered to be one of the simplest and most popular unsupervised machine learning techniques.\n",
    "Unsupervised algorithms use vectors on data points. These data points are not labeled or classified. Our goal is to discover hidden patterns and group the data points in a sensible way based on similarity of features. Each group of data points is a cluster and each cluster will have a center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18805ccd",
   "metadata": {},
   "source": [
    "## Process:\n",
    "\n",
    "* Pre-process the data (Clean it, Scale it, Standardize it)\n",
    "* Select K\n",
    "* Pick K Centers \n",
    "* Repeat until there is no change of the centroid positions: \n",
    "   1) Compute the distance between data point (vector x) and all centroids. (Generally, we use the euclidean distance) <img src=\"img/k_means_euclidean.png\" >\n",
    "   2) Assign each data point to the closest cluster (centroid) \n",
    "   3) Compute the centroids for the clusters by taking the average of all data points that belong to each cluster.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d426bff",
   "metadata": {},
   "source": [
    "<a name=\"application\"></a>\n",
    "<h2><span style=\"color:black\">K-Means Clustering Application: Building a diversified portfolio </span></h2>\n",
    "\n",
    "I am going to use K-Means Clustering to build a diversified portfolio. Two ratios will be used in order to cluster the data: <BR>\n",
    "<ul>\n",
    "    <li> <code>Revenue per share:</code> Amount of sales or revenues generated per average total shares outstanding. (Sales Revenue/Average Total Shares)</li>\n",
    "    <li><code>Return on Assets:</code> Indicator of how profitable company is relative to its assets (Total Income /Total Assets)</li>\n",
    "\n",
    "</ul>\n",
    "The idea is to create clusters with similar characteristics for the components of the S&P 500 using these two factors at the end of the 2021 Q1. From each cluster, we will take the stocks with highest risk adjusted momentum to build our portfolio. <br><br>\n",
    "   After building this portfolio, we will run it for 2021 Q2 and compare it to the return of the S&P 500.<br><br>\n",
    "    \n",
    "Please note that this analysis is done using only two factors which leads to a two dimensional problem. I will be using a two dimensional problem to demonstrate the concept and understand the problem. Multiple factors can be used as well. \n",
    "   \n",
    "I will proceed with the following steps: \n",
    "\n",
    "    1. Get the data: Revenue per share and Return on Assets for the end of 2021 Q1 for members of the S&P 500.\n",
    "    2. Analyze the data, clean it and visualize it.\n",
    "    3. Choose K.\n",
    "    4. Analyze the clustering results.\n",
    "<b>Portfolio Construction :\n",
    "    1. From each cluster, choose the stocks with the highest risk adjusted momentum. \n",
    "    2. Run the portfolio return for 2021-Q2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d489dce4",
   "metadata": {},
   "source": [
    "----\n",
    "Let's apply the steps defined above:\n",
    "## K-Means Clustering \n",
    "### <I>1. Get the data: Revenue per share and Return on Assets for the end of 2021 Q1 for members of the S&P 500</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d25296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the S&P 500 tickers from Wikipedia\n",
    "\n",
    "def get_tickers():\n",
    "    wiki_page = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies').text\n",
    "    sp_data = pd.read_html(wiki_page)\n",
    "    ticker_df = sp_data[0]\n",
    "    ticker_options = ticker_df['Symbol']\n",
    "    return ticker_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d49cc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "# Run the ticker scrape function\n",
    "# Let's convert the get_tickers() output to a list and \n",
    "# replace tickers that have '.' with '-' so we can use AlphaWave Data APIs\n",
    "\n",
    "stock_tickers = get_tickers()\n",
    "stock_tickers = stock_tickers.to_list()\n",
    "for ticker in range(len(stock_tickers)):\n",
    "    stock_tickers[ticker] = stock_tickers[ticker].upper().replace(\".\", \"-\")\n",
    "\n",
    "print (len(stock_tickers))\n",
    "# stock_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c021c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\proma.gupta\\AppData\\Local\\Temp\\ipykernel_15316\\2453419208.py:9: DeprecationWarning: Using 'method_whitelist' with Retry is deprecated and will be removed in v2.0. Use 'allowed_methods' instead\n",
      "  retry_strategy = Retry(total=3, backoff_factor=10, status_forcelist=[429, 500, 502, 503, 504], method_whitelist=[\"HEAD\", \"GET\", \"PUT\", \"DELETE\", \"OPTIONS\", \"TRACE\"])\n",
      "Retrieving AlphaWave Data Stock Info: 100%|██████████████████████████████████████████| 503/503 [25:13<00:00,  3.01s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphawave_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m data\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    401\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Fetch AlphaWave Data's fundamental stock information\n",
    "key_stats_url = \"https://stock-analysis.p.rapidapi.com/api/v1/resources/key-stats\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"YOUR_X-RAPIDAPI-HOST_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS\",\n",
    "    'x-rapidapi-key': \"YOUR_X-RAPIDAPI-KEY_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS\"\n",
    "    }\n",
    "\n",
    "retry_strategy = Retry(total=3, backoff_factor=10, status_forcelist=[429, 500, 502, 503, 504], method_whitelist=[\"HEAD\", \"GET\", \"PUT\", \"DELETE\", \"OPTIONS\", \"TRACE\"])\n",
    "rapid_api_adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "http = requests.Session()\n",
    "http.mount(\"https://\", rapid_api_adapter)\n",
    "\n",
    "alphawave_data = []\n",
    "\n",
    "for ticker in tqdm(stock_tickers, position=0, leave=True, desc = \"Retrieving AlphaWave Data Stock Info\"):\n",
    "    \n",
    "    querystring = {\"ticker\":ticker}\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Get Key Stats\n",
    "        key_stats_response = http.get(key_stats_url, headers=key_stats_headers, params=querystring, timeout=(5, 5))\n",
    "        key_stats_response.raise_for_status()\n",
    "        key_stats_df = pd.DataFrame.from_dict(key_stats_response.json())\n",
    "        key_stats_df = key_stats_df.transpose()\n",
    "\n",
    "        roa = key_stats_df.loc[r'Return on assets (ttm)'][0]\n",
    "        rev_per_share = key_stats_df.loc[r'Revenue per share (ttm)'][0]\n",
    "\n",
    "        # Create Dataframe\n",
    "        df = pd.DataFrame({'Return on Assets': roa,\n",
    "                           'Rev per share': rev_per_share},\n",
    "                          index=[ticker])\n",
    "\n",
    "        alphawave_data.append(df)\n",
    "\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print (\"OOps: Something Else\",err)\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print (\"Http Error:\",errh)\n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print (\"Error Connecting:\",errc)\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print (\"Timeout Error:\",errt)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "data = pd.concat(alphawave_data, ignore_index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ffd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any % characters, change string values to numeric values\n",
    "data[[\"Return on Assets\"]] = data[[\"Return on Assets\"]].apply(lambda x: x.str.replace('[%]','', regex=True))\n",
    "data[[\"Return on Assets\", \n",
    "      \"Rev per share\"]] = data[[\"Return on Assets\", \n",
    "                                   \"Rev per share\"]].apply(pd.to_numeric)\n",
    "data[[\"Return on Assets\"]] = data[[\"Return on Assets\"]].apply(lambda x: x/100)\n",
    "data.index.name = 'ID'\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72714e4b",
   "metadata": {},
   "source": [
    "### <I>2. Analyze the data, clean it and visualize it.</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079945be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original data before starting our data preprocessing\n",
    "original_data=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0adaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Na Values\n",
    "data[data['Return on Assets'].isna() | data['Rev per share'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef3fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropna value \n",
    "data=data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c20113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatterplot\n",
    "plt.style.use(\"dark_background\")\n",
    "g = sns.scatterplot(x='Return on Assets', y='Rev per share', data=data)\n",
    "plt.ylim([0,200])\n",
    "plt.title(\"Original Data\")\n",
    "\n",
    "# Some random point we want to classify\n",
    "plt.scatter(0.05, 50, marker='o', s=80, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both Revenue per share and Return on Assets are ratios. They are already scaled to the company size.\n",
    "# We can use Winsorization to transforms data by limiting extreme values, typically by setting all outliers to a specified percentile of data\n",
    "X =np.asarray([np.asarray(data['Return on Assets']),np.asarray(data['Rev per share'])])\n",
    "X = mstats.winsorize(X, limits = [0.05, 0.05])\n",
    "data=pd.DataFrame(X, index=['Return on Assets','Rev per share'], columns=data.index).T\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18776dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatterplot\n",
    "plt.style.use(\"dark_background\")\n",
    "g = sns.scatterplot(x='Return on Assets', y='Rev per share', data=data)\n",
    "plt.title(\"Winsorized Data\")\n",
    "\n",
    "# Some random point we want to classify\n",
    "plt.scatter(0.05, 50, marker='o', s=80, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ace952",
   "metadata": {},
   "source": [
    "### <I>3. Choose K</I>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d62745",
   "metadata": {},
   "source": [
    "The two most common methods to choose K ( the appropriate number of clusters) are :\n",
    "    <ul>\n",
    "        <li>The silhouette Coefficient</li>\n",
    "        <li>The Elbow Method </li>\n",
    "    </ul>\n",
    "\n",
    "The silhouette coefficient is a value that ranges between -1 and 1. It quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "1. How close the data point is to other points in the cluster\n",
    "2. How far away the data point is from points in other clusters\n",
    "\n",
    "Larger numbers for Silhouette coefficient indicate that samples are closer to their clusters than they are to other clusters.\n",
    "\n",
    "The elbow method is used by running several k-means, increment k with each iteration, and record the SSE ( Sum Of Squared Error) <br><br>\n",
    "$$SSE= Sum  \\; Of  \\; Euclidean  \\; Squared  \\; Distances  \\; of  \\; each  \\; point \\; to \\; its  \\; closest \\; centroid $$<br>\n",
    "After that , we plot SSE as a function of the number of clusters. SSE continues to decrease as you increase k. As more centroids are added, the distance from each point to its closest centroid will decrease.\n",
    "There’s a sweet spot where the SSE curve starts to bend known as the elbow point. The x-value of this point is thought to be a reasonable trade-off between error and number of clusters. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "distorsions = []\n",
    "clusters_iterations=range(2, 20)\n",
    "for k in clusters_iterations:\n",
    "    k_means = KMeans(n_clusters=k)\n",
    "    k_means.fit(data)\n",
    "    distorsions.append(k_means.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_curve_data=pd.DataFrame(zip(clusters_iterations,distorsions),columns=['Cluster','SSE']).set_index('Cluster')\n",
    "elbow_curve_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plot\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.plot(elbow_curve_data['SSE'])\n",
    "plt.title(\"Elbow Curve\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get elbow programmatically\n",
    "from kneed import KneeLocator \n",
    "kl = KneeLocator(\n",
    "clusters_iterations, distorsions, curve=\"convex\", direction=\"decreasing\")\n",
    "elbow=kl.elbow\n",
    "\n",
    "print('Elbow = {}'.format(elbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62f8153",
   "metadata": {},
   "source": [
    "### <I>4. Analyze the clustering results</I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply KMeans for the Elbow's value  ( in this case = 5)\n",
    "kmeans = KMeans(n_clusters=elbow)\n",
    "kmeans.fit(data)\n",
    "y_kmeans = kmeans.predict(data)\n",
    "df_kmeans = data.copy()\n",
    "df_kmeans['cluster']=y_kmeans.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.style.use(\"dark_background\")\n",
    "g = sns.scatterplot(x='Return on Assets', y='Rev per share', hue=df_kmeans['cluster'].astype(int), \n",
    "                    palette=['blue','green','yellow','orange','red'], data=df_kmeans)\n",
    "plt.title(\"K-Means Clustering\")\n",
    "\n",
    "# Some random point we want to classify\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the centers \n",
    "clusters_centers_df=pd.DataFrame(kmeans.cluster_centers_,columns=['Return on Assets','Rev per share'])\n",
    "clusters_centers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the clustering by Company \n",
    "clustering_result=pd.DataFrame(zip(y_kmeans,data.index),columns=['Cluster','Company'])\n",
    "clustering_result.set_index('Cluster').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3148a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_num in list(clustering_result.set_index('Cluster').index.unique()):\n",
    "    print (clustering_result.set_index('Cluster').loc[cluster_num].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich Centers Df with the number of elements by Cluster\n",
    "clusters_centers_df['Count']=clustering_result['Cluster'].value_counts().to_frame().rename(columns={'Cluster':'Count'})['Count']\n",
    "clusters_centers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e48dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Count of Elements by Cluster \n",
    "plt.figure(figsize=(11,7))\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.bar(clusters_centers_df.index.values,clusters_centers_df['Count'])\n",
    "plt.title(\"Count of Elements by Cluster\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6e695",
   "metadata": {},
   "source": [
    "## Portfolio Construction\n",
    "### <I>1. From each cluster, choose the stocks with the highest Risk Adjusted Momentum </I>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16762cfa",
   "metadata": {},
   "source": [
    "We can use the [2 Year Historical Daily Prices](https://rapidapi.com/alphawave/api/stock-prices2?endpoint=apiendpoint_33fa1878-1727-4775-beeb-f6b0da5314fd) endpoint from the [AlphaWave Data Stock Prices API](https://rapidapi.com/alphawave/api/stock-prices2/endpoints) to pull in the two year historical prices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch 2 year daily return data\n",
    "\n",
    "url = \"https://stock-prices2.p.rapidapi.com/api/v1/resources/stock-prices/2y\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"YOUR_X-RAPIDAPI-HOST_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS\",\n",
    "    'x-rapidapi-key': \"YOUR_X-RAPIDAPI-KEY_WILL_COPY_DIRECTLY_FROM_RAPIDAPI_PYTHON_CODE_SNIPPETS\"\n",
    "    }\n",
    "\n",
    "stock_frames = []\n",
    "\n",
    "# for ticker in stock_tickers:\n",
    "for ticker in tqdm(stock_tickers, position=0, leave=True, desc = \"Retrieving AlphaWave Data Stock Info\"):\n",
    "    \n",
    "    querystring = {\"ticker\":ticker}\n",
    "    stock_daily_price_response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "    # Create Stock Prices DataFrame\n",
    "    stock_daily_price_df = pd.DataFrame.from_dict(stock_daily_price_response.json())\n",
    "    stock_daily_price_df = stock_daily_price_df.transpose()\n",
    "    stock_daily_price_df = stock_daily_price_df.rename(columns={'Close':ticker})\n",
    "    stock_daily_price_df = stock_daily_price_df[{ticker}]\n",
    "    stock_frames.append(stock_daily_price_df)\n",
    "\n",
    "combined_stock_price_df = pd.concat(stock_frames, axis=1, sort=True)\n",
    "combined_stock_price_df = combined_stock_price_df.dropna(how='all')\n",
    "combined_stock_price_df = combined_stock_price_df.fillna(\"\")\n",
    "combined_stock_price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build of Portfolio of 50 stocks\n",
    "number_of_stocks=50\n",
    "\n",
    "# From each Cluster, we will pick the stocks with the highest risk adjusted momentum. The number of stocks from each cluster is proportional to its size\n",
    "# Let's start by calculate the number of stocks to pick from each cluster\n",
    "number_of_stocks_by_cluster=pd.DataFrame(round(number_of_stocks*clustering_result.groupby(by='Cluster').count()['Company']/clustering_result.count()['Company'],0))\n",
    "number_of_stocks_by_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75342fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From each Cluster, pick the stocks with the highest risk adjusted momentum.\n",
    "as_of_date='2021-03-30'\n",
    "\n",
    "portfolio_stocks=[]\n",
    "for cluster_num in list(number_of_stocks_by_cluster.index):\n",
    "    # for each cluster,get all the companies within this cluster\n",
    "    list_stocks=list(clustering_result.set_index('Cluster').loc[cluster_num]['Company'])\n",
    "    #get the number of stocks that we will pick for our portfolio     \n",
    "    number_stocks=number_of_stocks_by_cluster.loc[cluster_num]['Company']\n",
    "    if number_stocks>0:\n",
    "        # Compute the risk adjusted momentum for the past year\n",
    "        last_year_date=pd.to_datetime(as_of_date)+ pd.offsets.DateOffset(years=-1)\n",
    "        last_month_date=pd.to_datetime(as_of_date)+ pd.tseries.offsets.BusinessDay(offset = timedelta(days = -30))\n",
    "        stock_price_last_year_date = last_year_date.strftime('%Y-%m-%d')\n",
    "        stock_price_last_month_date = last_month_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        risk_adjusted_mom_frames = []\n",
    "        for ticker in list_stocks:\n",
    "\n",
    "            try:\n",
    "                momentum = (combined_stock_price_df.loc[stock_price_last_month_date,][ticker] - \\\n",
    "                            combined_stock_price_df.loc[stock_price_last_year_date,][ticker]) / \\\n",
    "                            combined_stock_price_df.loc[stock_price_last_year_date,][ticker]\n",
    "\n",
    "                annualized_volatility = np.log(combined_stock_price_df.loc[stock_price_last_year_date:as_of_date,][ticker] / \\\n",
    "                                               combined_stock_price_df.loc[stock_price_last_year_date:as_of_date,][ticker].shift(1)).dropna().std()*252**.5\n",
    "\n",
    "                risk_adjusted_momentum = momentum / annualized_volatility\n",
    "\n",
    "                # Create Dataframe\n",
    "                df = pd.DataFrame({'Risk Adj MoM': risk_adjusted_momentum},\n",
    "                                  index=[ticker])\n",
    "\n",
    "                risk_adjusted_mom_frames.append(df)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        risk_adjusted_mom_df = pd.concat(risk_adjusted_mom_frames, ignore_index=False)\n",
    "        risk_adjusted_mom_df[\"Rank\"] = risk_adjusted_mom_df[\"Risk Adj MoM\"].rank(ascending=False)\n",
    "        risk_adjusted_mom_df[[\"Risk Adj MoM\", \n",
    "                              \"Rank\"]] = risk_adjusted_mom_df[[\"Risk Adj MoM\", \n",
    "                                                               \"Rank\"]].apply(pd.to_numeric)\n",
    "        filtered_risk_adjusted_mom_df = risk_adjusted_mom_df[risk_adjusted_mom_df['Rank'] <= number_stocks]\n",
    "        portfolio_stocks=portfolio_stocks+list(filtered_risk_adjusted_mom_df.index)\n",
    "\n",
    "portfolio_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b048d",
   "metadata": {},
   "source": [
    "### <I> 2. Compute Portfolio's Performance for 2021-Q2 </I>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we chose our portfolio stocks by end the of 2021-Q1, we will run it for 2021-Q2 \n",
    "end_date='2021-06-30'\n",
    "\n",
    "# Compute the portfolio return. We will use equal weights for all the stocks\n",
    "combined_stock_price_df = combined_stock_price_df.apply(pd.to_numeric)\n",
    "s_p_500_daily_return = (combined_stock_price_df.loc[as_of_date:end_date,].pct_change().sum(axis=1).dropna()/len(combined_stock_price_df.columns)) + 1\n",
    "cluster_portfolio_return=0\n",
    "for stock in portfolio_stocks:\n",
    "    daily_return = combined_stock_price_df.loc[as_of_date:end_date,][stock].pct_change().dropna() + 1\n",
    "    cluster_portfolio_return=cluster_portfolio_return+(daily_return/len(portfolio_stocks))\n",
    "\n",
    "# Create Dataframe\n",
    "df = pd.DataFrame({'cluster_portfolio_return':cluster_portfolio_return,\n",
    "                   'spx_index_return':s_p_500_daily_return},)\n",
    "\n",
    "df.index.name = 'DATE'\n",
    "return_ptf_index = df.dropna()\n",
    "return_ptf_index = return_ptf_index.apply(pd.to_numeric)\n",
    "\n",
    "return_ptf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the annual volatility, sharpe ratio and annual excess return and plot the cumulative return\n",
    "from math import sqrt\n",
    "\n",
    "# compute the timeline for annualization\n",
    "T = (pd.to_datetime(return_ptf_index['cluster_portfolio_return'].index[-1]) - pd.to_datetime(return_ptf_index['cluster_portfolio_return'].index[0])) / np.timedelta64(1, 'Y')\n",
    "\n",
    "#portfolio Excess Return\n",
    "portfolio_excess_return=round(100*(return_ptf_index['cluster_portfolio_return'].cumprod().iloc[-1]**(1/T) - 1),2)\n",
    "\n",
    "#Portfolio Annual Volatility\n",
    "portfolio_annual_volatility=round(100*return_ptf_index['cluster_portfolio_return'].std()*sqrt(252),2)\n",
    "\n",
    "#Portfolio Sharpe Ratio\n",
    "portfolio_sharpe_ratio=round((portfolio_excess_return)/portfolio_annual_volatility,2)\n",
    "\n",
    "# Plot Results\n",
    "print (\"Portfolio Annual Excess Return : {}%\".format(portfolio_excess_return))\n",
    "print (\"Portfolio Annual Volatility    : {}% \".format(portfolio_annual_volatility))\n",
    "print (\"Portfolio Sharpe Ratio         : {}\".format(portfolio_sharpe_ratio)) \n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "ax = plt.gca()\n",
    "plt.title(\"Portfolio Performance\")\n",
    "return_ptf_index['cluster_portfolio_return'].cumprod().plot(ax=ax,color=sns.color_palette()[1],linewidth=2)\n",
    "return_ptf_index['spx_index_return'].cumprod().plot(ax=ax,color=sns.color_palette()[2],linewidth=2)\n",
    "plt.ylabel(\"Cumulative Return %\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3fc46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
